{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d695e2-531a-4c82-a3fd-147fb9718230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=PaCmpygFfXo\n",
    "\n",
    "# Learning to make Makemore by Andrej Karpathy!\n",
    "# Makemore is a Character Level Language Model\n",
    "# Treats every input as a sequence of individual characters\n",
    "# predict next character in the sequence\n",
    "# Using Neural Network approach now with training set splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68ba4b89-e99a-4231-9f7a-550c3d378390",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4df9c463-c14f-43e6-9233-7484dac7b3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d91cbc8-502c-487c-b34f-ef9254036ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((27, 27), dtype=torch.int32) # counts of letters and start and end characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c67548db-3362-48b6-b6e4-93d8e750b892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 0: '.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d43964b3-2eb5-4a54-8c57-d50c71b79655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training set of bigrams (x, y)\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words[:1]:\n",
    "    chs = ['.'] + list(w) + ['.'] # accounting for start and end characters\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs) # infers datatype, torch.Tensor(xs) creates floats\n",
    "ys = torch.tensor(ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4cc0b87-d694-438e-bf84-f025728e6de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40bb8ecf-135b-46fb-8de0-9031b138f064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90fa49d4-1a0d-4725-882f-533bb5581441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encoding\n",
    "# turns the 13 into a vector with all 0s besides the 13th index being a 1\n",
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48589ea3-b5c9-496a-9adc-1c30cf496cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1877ed21bd0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f57bf734-1bd8-44a3-9ad0-075b6688ad4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0679,  0.1867, -0.6524,  1.0157, -1.1255, -0.2826,  0.2065, -1.5118,\n",
       "          1.6391,  0.5119,  0.6136,  0.8731, -0.1425,  1.1610,  0.3290,  0.1127,\n",
       "          0.5425,  1.0464,  0.1613,  1.4677, -1.8029,  1.1601, -0.5343,  0.3071,\n",
       "          1.1355,  0.8755, -2.4943],\n",
       "        [-0.7809, -0.0359, -3.0685,  0.4164,  0.2315, -0.4644, -0.9839,  0.0091,\n",
       "         -0.9283, -0.4792, -0.9274, -0.4708,  1.7571,  0.5549, -0.2523,  0.8310,\n",
       "          0.2802,  0.5157, -0.8587, -1.0831, -0.3344, -0.1858, -1.5041,  0.4876,\n",
       "         -0.0537,  0.7809, -1.5634],\n",
       "        [ 0.8052,  0.6972, -0.7605,  1.3057,  1.5494, -1.0209,  0.4867,  2.4423,\n",
       "         -0.4113, -0.0797, -0.8247,  0.4833, -1.5567,  0.3247, -0.3154, -0.3230,\n",
       "          0.3733,  0.8861, -0.6105,  0.6191, -0.0485,  1.0038,  0.0443, -0.7961,\n",
       "         -0.3803,  0.3231,  1.2685],\n",
       "        [ 0.8052,  0.6972, -0.7605,  1.3057,  1.5494, -1.0209,  0.4867,  2.4423,\n",
       "         -0.4113, -0.0797, -0.8247,  0.4833, -1.5567,  0.3247, -0.3154, -0.3230,\n",
       "          0.3733,  0.8861, -0.6105,  0.6191, -0.0485,  1.0038,  0.0443, -0.7961,\n",
       "         -0.3803,  0.3231,  1.2685],\n",
       "        [ 0.5139, -0.6423,  1.4021, -1.2136,  1.1957,  0.9599, -1.3028,  0.7914,\n",
       "         -0.4757, -1.4713, -0.4847,  0.6362, -0.2707,  0.1510,  0.5641, -1.5722,\n",
       "         -0.2255, -0.8277, -0.3746,  1.1360,  1.1221, -1.7362,  0.8702, -0.2102,\n",
       "         -0.3419, -0.2281, -1.1719]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.randn((27, 27)) \n",
    "# fills the tensor with random numbers based on normal distribution\n",
    "\n",
    "(xenc @ W) # matrix multiplication: (5, 27) x (27, 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d5fcfc10-2feb-4018-b1d5-25f77493e75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1477, 0.0225, 0.0097, 0.0516, 0.0061, 0.0141, 0.0230, 0.0041, 0.0962,\n",
       "         0.0312, 0.0345, 0.0447, 0.0162, 0.0597, 0.0260, 0.0209, 0.0321, 0.0532,\n",
       "         0.0220, 0.0811, 0.0031, 0.0596, 0.0109, 0.0254, 0.0581, 0.0448, 0.0015],\n",
       "        [0.0156, 0.0328, 0.0016, 0.0516, 0.0429, 0.0214, 0.0127, 0.0343, 0.0134,\n",
       "         0.0211, 0.0135, 0.0212, 0.1971, 0.0592, 0.0264, 0.0781, 0.0450, 0.0570,\n",
       "         0.0144, 0.0115, 0.0243, 0.0282, 0.0076, 0.0554, 0.0322, 0.0743, 0.0071],\n",
       "        [0.0444, 0.0399, 0.0093, 0.0733, 0.0935, 0.0072, 0.0323, 0.2283, 0.0132,\n",
       "         0.0183, 0.0087, 0.0322, 0.0042, 0.0275, 0.0145, 0.0144, 0.0288, 0.0482,\n",
       "         0.0108, 0.0369, 0.0189, 0.0542, 0.0208, 0.0090, 0.0136, 0.0274, 0.0706],\n",
       "        [0.0444, 0.0399, 0.0093, 0.0733, 0.0935, 0.0072, 0.0323, 0.2283, 0.0132,\n",
       "         0.0183, 0.0087, 0.0322, 0.0042, 0.0275, 0.0145, 0.0144, 0.0288, 0.0482,\n",
       "         0.0108, 0.0369, 0.0189, 0.0542, 0.0208, 0.0090, 0.0136, 0.0274, 0.0706],\n",
       "        [0.0471, 0.0148, 0.1145, 0.0084, 0.0931, 0.0736, 0.0077, 0.0622, 0.0175,\n",
       "         0.0065, 0.0173, 0.0532, 0.0215, 0.0328, 0.0495, 0.0058, 0.0225, 0.0123,\n",
       "         0.0194, 0.0877, 0.0865, 0.0050, 0.0673, 0.0228, 0.0200, 0.0224, 0.0087]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = xenc @ W # log counts\n",
    "counts = logits.exp() # everything negative is lower than 1\n",
    "probs = counts / counts.sum(1, keepdim=True) # normalize counts\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e55038f2-7cd1-4f42-b2ac-d9799493f572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "bigram example 1: .e (indexes 0,5)\n",
      "input to the neural net: 0\n",
      "output probabilities from the neural net: tensor([0.1477, 0.0225, 0.0097, 0.0516, 0.0061, 0.0141, 0.0230, 0.0041, 0.0962,\n",
      "        0.0312, 0.0345, 0.0447, 0.0162, 0.0597, 0.0260, 0.0209, 0.0321, 0.0532,\n",
      "        0.0220, 0.0811, 0.0031, 0.0596, 0.0109, 0.0254, 0.0581, 0.0448, 0.0015])\n",
      "label (actual next character): 5\n",
      "probability assigned by the net to the the correct character: 0.01408227439969778\n",
      "log likelihood: -4.262838363647461\n",
      "negative log likelihood: 4.262838363647461\n",
      "--------\n",
      "bigram example 2: em (indexes 5,13)\n",
      "input to the neural net: 5\n",
      "output probabilities from the neural net: tensor([0.0156, 0.0328, 0.0016, 0.0516, 0.0429, 0.0214, 0.0127, 0.0343, 0.0134,\n",
      "        0.0211, 0.0135, 0.0212, 0.1971, 0.0592, 0.0264, 0.0781, 0.0450, 0.0570,\n",
      "        0.0144, 0.0115, 0.0243, 0.0282, 0.0076, 0.0554, 0.0322, 0.0743, 0.0071])\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.05924506112933159\n",
      "log likelihood: -2.826072931289673\n",
      "negative log likelihood: 2.826072931289673\n",
      "--------\n",
      "bigram example 3: mm (indexes 13,13)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0444, 0.0399, 0.0093, 0.0733, 0.0935, 0.0072, 0.0323, 0.2283, 0.0132,\n",
      "        0.0183, 0.0087, 0.0322, 0.0042, 0.0275, 0.0145, 0.0144, 0.0288, 0.0482,\n",
      "        0.0108, 0.0369, 0.0189, 0.0542, 0.0208, 0.0090, 0.0136, 0.0274, 0.0706])\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.027468916028738022\n",
      "log likelihood: -3.594700336456299\n",
      "negative log likelihood: 3.594700336456299\n",
      "--------\n",
      "bigram example 4: ma (indexes 13,1)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0444, 0.0399, 0.0093, 0.0733, 0.0935, 0.0072, 0.0323, 0.2283, 0.0132,\n",
      "        0.0183, 0.0087, 0.0322, 0.0042, 0.0275, 0.0145, 0.0144, 0.0288, 0.0482,\n",
      "        0.0108, 0.0369, 0.0189, 0.0542, 0.0208, 0.0090, 0.0136, 0.0274, 0.0706])\n",
      "label (actual next character): 1\n",
      "probability assigned by the net to the the correct character: 0.039868809282779694\n",
      "log likelihood: -3.222161054611206\n",
      "negative log likelihood: 3.222161054611206\n",
      "--------\n",
      "bigram example 5: a. (indexes 1,0)\n",
      "input to the neural net: 1\n",
      "output probabilities from the neural net: tensor([0.0471, 0.0148, 0.1145, 0.0084, 0.0931, 0.0736, 0.0077, 0.0622, 0.0175,\n",
      "        0.0065, 0.0173, 0.0532, 0.0215, 0.0328, 0.0495, 0.0058, 0.0225, 0.0123,\n",
      "        0.0194, 0.0877, 0.0865, 0.0050, 0.0673, 0.0228, 0.0200, 0.0224, 0.0087])\n",
      "label (actual next character): 0\n",
      "probability assigned by the net to the the correct character: 0.04709070920944214\n",
      "log likelihood: -3.0556795597076416\n",
      "negative log likelihood: 3.0556795597076416\n",
      "=========\n",
      "average negative log likelihood, i.e. loss = 3.3922901153564453\n"
     ]
    }
   ],
   "source": [
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "    # i-th bigram:\n",
    "    x = xs[i].item() # input character index\n",
    "    y = ys[i].item() # label character index\n",
    "    print('--------')\n",
    "    print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')\n",
    "    print('input to the neural net:', x)\n",
    "    print('output probabilities from the neural net:', probs[i])\n",
    "    print('label (actual next character):', y)\n",
    "    p = probs[i, y]\n",
    "    print('probability assigned by the net to the the correct character:', p.item())\n",
    "    logp = torch.log(p)\n",
    "    print('log likelihood:', logp.item())\n",
    "    nll = -logp\n",
    "    print('negative log likelihood:', nll.item())\n",
    "    nlls[i] = nll\n",
    "\n",
    "print('=========')\n",
    "print('average negative log likelihood, i.e. loss =', nlls.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccb9ba8-6f51-48be-b4ed-aa6663fa579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f42d7c24-1a0f-4a8a-89f3-5b2174111528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random 27 neuron weights\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7c24acd5-de10-4a73-9b3a-322144921d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the network\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0777ee87-6411-4686-a0bf-c463fb51b97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4818339347839355\n",
      "2.481825828552246\n",
      "2.4818174839019775\n",
      "2.481809139251709\n",
      "2.4818010330200195\n",
      "2.481793165206909\n",
      "2.481785297393799\n",
      "2.4817774295806885\n",
      "2.481769323348999\n",
      "2.481761932373047\n",
      "2.4817538261413574\n",
      "2.481745958328247\n",
      "2.481738328933716\n",
      "2.4817306995391846\n",
      "2.4817235469818115\n",
      "2.4817161560058594\n",
      "2.481708526611328\n",
      "2.481701135635376\n",
      "2.481694221496582\n",
      "2.48168683052063\n",
      "2.481680154800415\n",
      "2.481672763824463\n",
      "2.48166561126709\n",
      "2.481658697128296\n",
      "2.481651782989502\n",
      "2.481644868850708\n",
      "2.481637954711914\n",
      "2.481631278991699\n",
      "2.4816248416900635\n",
      "2.4816179275512695\n",
      "2.4816112518310547\n",
      "2.48160457611084\n",
      "2.4815986156463623\n",
      "2.4815919399261475\n",
      "2.4815855026245117\n",
      "2.481579303741455\n",
      "2.4815726280212402\n",
      "2.4815664291381836\n",
      "2.481560468673706\n",
      "2.4815540313720703\n",
      "2.4815480709075928\n",
      "2.481541872024536\n",
      "2.4815359115600586\n",
      "2.481529712677002\n",
      "2.4815237522125244\n",
      "2.4815175533294678\n",
      "2.4815120697021484\n",
      "2.481506109237671\n",
      "2.4815003871917725\n",
      "2.481494426727295\n",
      "2.4814889430999756\n",
      "2.481482982635498\n",
      "2.481477737426758\n",
      "2.4814717769622803\n",
      "2.48146653175354\n",
      "2.4814605712890625\n",
      "2.4814555644989014\n",
      "2.481449604034424\n",
      "2.4814443588256836\n",
      "2.4814391136169434\n",
      "2.481433629989624\n",
      "2.481428384780884\n",
      "2.4814231395721436\n",
      "2.4814178943634033\n",
      "2.481412649154663\n",
      "2.481407642364502\n",
      "2.4814023971557617\n",
      "2.4813973903656006\n",
      "2.4813923835754395\n",
      "2.4813876152038574\n",
      "2.481382369995117\n",
      "2.481377124786377\n",
      "2.481372594833374\n",
      "2.481367826461792\n",
      "2.481362819671631\n",
      "2.4813578128814697\n",
      "2.481353282928467\n",
      "2.4813485145568848\n",
      "2.4813437461853027\n",
      "2.4813389778137207\n",
      "2.4813344478607178\n",
      "2.4813296794891357\n",
      "2.481325149536133\n",
      "2.481320381164551\n",
      "2.481315851211548\n",
      "2.481311321258545\n",
      "2.481307029724121\n",
      "2.481302499771118\n",
      "2.4812982082366943\n",
      "2.4812934398651123\n",
      "2.4812893867492676\n",
      "2.4812848567962646\n",
      "2.481280565261841\n",
      "2.481276512145996\n",
      "2.4812722206115723\n",
      "2.4812681674957275\n",
      "2.4812636375427246\n",
      "2.48125958442688\n",
      "2.481255292892456\n",
      "2.4812512397766113\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(100):\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    \n",
    "    # Calculate loss with average negative log likelihood (nll)\n",
    "    # regularization loss to try to make all W's be close to 0 as well\n",
    "    # increasing the regularization coefficient basically is same as increasing count in smoothing\n",
    "    loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
    "    print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None # setting gradients to None is more efficient than zero_grad\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    #update\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e7e523f3-8715-4bb0-a0a5-9feb2841a680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "janasah.\n",
      "p.\n",
      "cfay.\n",
      "a.\n"
     ]
    }
   ],
   "source": [
    "# sample from the \"neural net\" model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "\n",
    "        # Before: p = P[x]\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum(1, keepdims=True)\n",
    "\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afee2b9-6797-402f-a955-57aacd339a15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
